{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eac6993-0e72-460d-81fa-6f02801695ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1: Train Loss 243.7425, Test Loss 204.0431\n",
      "Fold 1, Epoch 2: Train Loss 167.2036, Test Loss 102.4151\n",
      "Fold 1, Epoch 3: Train Loss 105.4399, Test Loss 96.0960\n",
      "Fold 1, Epoch 4: Train Loss 102.3840, Test Loss 93.1141\n",
      "Fold 1, Epoch 5: Train Loss 99.4453, Test Loss 92.4906\n",
      "Fold 1, Epoch 6: Train Loss 97.0529, Test Loss 86.1986\n",
      "Fold 1, Epoch 7: Train Loss 91.7267, Test Loss 80.1030\n",
      "Fold 1, Epoch 8: Train Loss 86.5610, Test Loss 79.8065\n",
      "Fold 1, Epoch 9: Train Loss 84.4327, Test Loss 75.0851\n",
      "Fold 1, Epoch 10: Train Loss 83.5114, Test Loss 73.8982\n",
      "Fold 1, Epoch 11: Train Loss 81.1430, Test Loss 72.4198\n",
      "Fold 1, Epoch 12: Train Loss 80.9970, Test Loss 72.5339\n",
      "Fold 1, Epoch 13: Train Loss 79.4513, Test Loss 70.7466\n",
      "Fold 1, Epoch 14: Train Loss 79.9100, Test Loss 73.1786\n",
      "Fold 1, Epoch 15: Train Loss 78.9962, Test Loss 70.2452\n",
      "Fold 1, Epoch 16: Train Loss 78.6866, Test Loss 68.9847\n",
      "Fold 1, Epoch 17: Train Loss 77.0082, Test Loss 70.3501\n",
      "Fold 1, Epoch 18: Train Loss 77.3292, Test Loss 69.2667\n",
      "Fold 1, Epoch 19: Train Loss 77.1085, Test Loss 68.1636\n",
      "Fold 1, Epoch 20: Train Loss 77.0157, Test Loss 67.7943\n",
      "Fold 1, Epoch 21: Train Loss 76.4099, Test Loss 67.8133\n",
      "Fold 1, Epoch 22: Train Loss 75.8262, Test Loss 68.9491\n",
      "Fold 1, Epoch 23: Train Loss 75.5656, Test Loss 67.7923\n",
      "Fold 1, Epoch 24: Train Loss 75.7050, Test Loss 68.4169\n",
      "Fold 1, Epoch 25: Train Loss 75.2178, Test Loss 69.1378\n",
      "Fold 1, Epoch 26: Train Loss 75.5049, Test Loss 67.3816\n",
      "Fold 1, Epoch 27: Train Loss 74.2486, Test Loss 67.5377\n",
      "Fold 1, Epoch 28: Train Loss 75.2490, Test Loss 67.4261\n",
      "Fold 1, Epoch 29: Train Loss 75.2870, Test Loss 67.9748\n",
      "Fold 1, Epoch 30: Train Loss 75.4201, Test Loss 68.2604\n",
      "Fold 1, Epoch 31: Train Loss 73.8264, Test Loss 66.4352\n",
      "Fold 1, Epoch 32: Train Loss 73.9639, Test Loss 65.9466\n",
      "Fold 1, Epoch 33: Train Loss 72.8323, Test Loss 65.2758\n",
      "Fold 1, Epoch 34: Train Loss 73.6679, Test Loss 66.1216\n",
      "Fold 1, Epoch 35: Train Loss 72.7199, Test Loss 67.6579\n",
      "Fold 1, Epoch 36: Train Loss 72.5797, Test Loss 64.9297\n",
      "Fold 1, Epoch 37: Train Loss 72.4497, Test Loss 66.1840\n",
      "Fold 1, Epoch 38: Train Loss 72.2333, Test Loss 65.5753\n",
      "Fold 1, Epoch 39: Train Loss 72.2697, Test Loss 66.3252\n",
      "Fold 1, Epoch 40: Train Loss 71.8025, Test Loss 64.0059\n",
      "Fold 1, Epoch 41: Train Loss 71.3004, Test Loss 68.7553\n",
      "Fold 1, Epoch 42: Train Loss 71.4204, Test Loss 65.0170\n",
      "Fold 1, Epoch 43: Train Loss 71.0339, Test Loss 64.3639\n",
      "Fold 1, Epoch 44: Train Loss 70.8663, Test Loss 63.9417\n",
      "Fold 1, Epoch 45: Train Loss 70.4257, Test Loss 65.0361\n",
      "Fold 1, Epoch 46: Train Loss 70.5140, Test Loss 63.7984\n",
      "Fold 1, Epoch 47: Train Loss 70.5353, Test Loss 63.4333\n",
      "Fold 1, Epoch 48: Train Loss 69.5147, Test Loss 63.4499\n",
      "Fold 1, Epoch 49: Train Loss 70.4330, Test Loss 63.0805\n",
      "Fold 1, Epoch 50: Train Loss 68.5650, Test Loss 63.5986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/1834357499.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - MAE: 5.5022, RMSE: 7.9393\n",
      "Fold 2, Epoch 1: Train Loss 244.8329, Test Loss 213.6348\n",
      "Fold 2, Epoch 2: Train Loss 172.5303, Test Loss 114.3899\n",
      "Fold 2, Epoch 3: Train Loss 104.6130, Test Loss 103.3774\n",
      "Fold 2, Epoch 4: Train Loss 101.3671, Test Loss 101.3274\n",
      "Fold 2, Epoch 5: Train Loss 99.9052, Test Loss 100.4946\n",
      "Fold 2, Epoch 6: Train Loss 98.1777, Test Loss 98.0138\n",
      "Fold 2, Epoch 7: Train Loss 96.4765, Test Loss 96.6921\n",
      "Fold 2, Epoch 8: Train Loss 95.0920, Test Loss 92.2000\n",
      "Fold 2, Epoch 9: Train Loss 89.8422, Test Loss 85.8883\n",
      "Fold 2, Epoch 10: Train Loss 85.4669, Test Loss 80.5623\n",
      "Fold 2, Epoch 11: Train Loss 82.1017, Test Loss 78.8288\n",
      "Fold 2, Epoch 12: Train Loss 79.6705, Test Loss 77.9753\n",
      "Fold 2, Epoch 13: Train Loss 79.2478, Test Loss 79.2422\n",
      "Fold 2, Epoch 14: Train Loss 78.1880, Test Loss 80.8158\n",
      "Fold 2, Epoch 15: Train Loss 78.0260, Test Loss 74.8930\n",
      "Fold 2, Epoch 16: Train Loss 77.3461, Test Loss 77.6935\n",
      "Fold 2, Epoch 17: Train Loss 76.4720, Test Loss 74.7878\n",
      "Fold 2, Epoch 18: Train Loss 75.6229, Test Loss 74.2530\n",
      "Fold 2, Epoch 19: Train Loss 75.4411, Test Loss 73.2924\n",
      "Fold 2, Epoch 20: Train Loss 75.1023, Test Loss 76.7311\n",
      "Fold 2, Epoch 21: Train Loss 75.3408, Test Loss 74.7335\n",
      "Fold 2, Epoch 22: Train Loss 74.9791, Test Loss 74.0939\n",
      "Fold 2, Epoch 23: Train Loss 74.3938, Test Loss 74.0258\n",
      "Fold 2, Epoch 24: Train Loss 74.3611, Test Loss 73.7828\n",
      "Fold 2, Epoch 25: Train Loss 73.6226, Test Loss 76.8302\n",
      "Fold 2, Epoch 26: Train Loss 73.7574, Test Loss 71.8442\n",
      "Fold 2, Epoch 27: Train Loss 73.5890, Test Loss 72.2921\n",
      "Fold 2, Epoch 28: Train Loss 72.6968, Test Loss 73.5914\n",
      "Fold 2, Epoch 29: Train Loss 73.8154, Test Loss 72.0864\n",
      "Fold 2, Epoch 30: Train Loss 73.1250, Test Loss 72.6959\n",
      "Fold 2, Epoch 31: Train Loss 73.1220, Test Loss 71.5444\n",
      "Fold 2, Epoch 32: Train Loss 72.3601, Test Loss 71.5121\n",
      "Fold 2, Epoch 33: Train Loss 72.0717, Test Loss 70.8584\n",
      "Fold 2, Epoch 34: Train Loss 72.3760, Test Loss 73.3361\n",
      "Fold 2, Epoch 35: Train Loss 71.9106, Test Loss 72.7050\n",
      "Fold 2, Epoch 36: Train Loss 71.8922, Test Loss 74.1598\n",
      "Fold 2, Epoch 37: Train Loss 71.5492, Test Loss 70.3574\n",
      "Fold 2, Epoch 38: Train Loss 71.5262, Test Loss 70.0132\n",
      "Fold 2, Epoch 39: Train Loss 71.3605, Test Loss 71.0815\n",
      "Fold 2, Epoch 40: Train Loss 70.3965, Test Loss 71.9726\n",
      "Fold 2, Epoch 41: Train Loss 71.2621, Test Loss 71.5732\n",
      "Fold 2, Epoch 42: Train Loss 70.3135, Test Loss 71.7661\n",
      "Fold 2, Epoch 43: Train Loss 70.5620, Test Loss 72.5767\n",
      "Fold 2, Epoch 44: Train Loss 70.6267, Test Loss 71.0410\n",
      "Fold 2, Epoch 45: Train Loss 69.8510, Test Loss 70.0775\n",
      "Fold 2, Epoch 46: Train Loss 70.5588, Test Loss 72.5217\n",
      "Fold 2, Epoch 47: Train Loss 70.1649, Test Loss 71.7848\n",
      "Fold 2, Epoch 48: Train Loss 69.7839, Test Loss 70.4186\n",
      "Early stopping...\n",
      "Fold 2 - MAE: 5.7464, RMSE: 8.3659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/1834357499.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, Epoch 1: Train Loss 242.9917, Test Loss 203.4605\n",
      "Fold 3, Epoch 2: Train Loss 179.4683, Test Loss 107.3719\n",
      "Fold 3, Epoch 3: Train Loss 105.7649, Test Loss 97.0190\n",
      "Fold 3, Epoch 4: Train Loss 101.4395, Test Loss 93.5526\n",
      "Fold 3, Epoch 5: Train Loss 98.6810, Test Loss 90.8478\n",
      "Fold 3, Epoch 6: Train Loss 95.1589, Test Loss 85.9514\n",
      "Fold 3, Epoch 7: Train Loss 88.8612, Test Loss 78.5959\n",
      "Fold 3, Epoch 8: Train Loss 85.2375, Test Loss 75.0162\n",
      "Fold 3, Epoch 9: Train Loss 83.6448, Test Loss 75.2276\n",
      "Fold 3, Epoch 10: Train Loss 80.8715, Test Loss 72.8702\n",
      "Fold 3, Epoch 11: Train Loss 80.4238, Test Loss 71.3618\n",
      "Fold 3, Epoch 12: Train Loss 80.0994, Test Loss 72.0677\n",
      "Fold 3, Epoch 13: Train Loss 78.3625, Test Loss 69.7504\n",
      "Fold 3, Epoch 14: Train Loss 78.2071, Test Loss 69.2664\n",
      "Fold 3, Epoch 15: Train Loss 78.0431, Test Loss 70.2540\n",
      "Fold 3, Epoch 16: Train Loss 77.6895, Test Loss 69.5682\n",
      "Fold 3, Epoch 17: Train Loss 76.8096, Test Loss 69.8792\n",
      "Fold 3, Epoch 18: Train Loss 76.7081, Test Loss 68.5512\n",
      "Fold 3, Epoch 19: Train Loss 76.9369, Test Loss 68.3118\n",
      "Fold 3, Epoch 20: Train Loss 75.8766, Test Loss 69.4747\n",
      "Fold 3, Epoch 21: Train Loss 76.1234, Test Loss 68.4033\n",
      "Fold 3, Epoch 22: Train Loss 75.6855, Test Loss 67.5176\n",
      "Fold 3, Epoch 23: Train Loss 75.2005, Test Loss 67.7063\n",
      "Fold 3, Epoch 24: Train Loss 74.3946, Test Loss 70.2663\n",
      "Fold 3, Epoch 25: Train Loss 73.9745, Test Loss 67.9408\n",
      "Fold 3, Epoch 26: Train Loss 73.3789, Test Loss 67.5101\n",
      "Fold 3, Epoch 27: Train Loss 72.9137, Test Loss 65.9660\n",
      "Fold 3, Epoch 28: Train Loss 73.3399, Test Loss 69.1204\n",
      "Fold 3, Epoch 29: Train Loss 73.3786, Test Loss 66.5920\n",
      "Fold 3, Epoch 30: Train Loss 72.3512, Test Loss 66.9933\n",
      "Fold 3, Epoch 31: Train Loss 71.8631, Test Loss 66.9065\n",
      "Fold 3, Epoch 32: Train Loss 72.0077, Test Loss 65.9636\n",
      "Fold 3, Epoch 33: Train Loss 72.0294, Test Loss 65.4156\n",
      "Fold 3, Epoch 34: Train Loss 71.9417, Test Loss 65.3800\n",
      "Fold 3, Epoch 35: Train Loss 71.8650, Test Loss 66.6662\n",
      "Fold 3, Epoch 36: Train Loss 71.9424, Test Loss 64.1368\n",
      "Fold 3, Epoch 37: Train Loss 70.3915, Test Loss 65.2953\n",
      "Fold 3, Epoch 38: Train Loss 70.3172, Test Loss 64.9763\n",
      "Fold 3, Epoch 39: Train Loss 70.7793, Test Loss 65.1385\n",
      "Fold 3, Epoch 40: Train Loss 70.1290, Test Loss 64.9666\n",
      "Fold 3, Epoch 41: Train Loss 69.9965, Test Loss 64.4139\n",
      "Fold 3, Epoch 42: Train Loss 69.4916, Test Loss 64.3922\n",
      "Fold 3, Epoch 43: Train Loss 68.5708, Test Loss 65.4814\n",
      "Fold 3, Epoch 44: Train Loss 69.2924, Test Loss 63.4496\n",
      "Fold 3, Epoch 45: Train Loss 68.9599, Test Loss 63.0829\n",
      "Fold 3, Epoch 46: Train Loss 69.0669, Test Loss 63.9887\n",
      "Fold 3, Epoch 47: Train Loss 68.5268, Test Loss 64.5022\n",
      "Fold 3, Epoch 48: Train Loss 68.1973, Test Loss 64.3835\n",
      "Fold 3, Epoch 49: Train Loss 68.1286, Test Loss 63.9522\n",
      "Fold 3, Epoch 50: Train Loss 68.0678, Test Loss 63.2937\n",
      "Fold 3 - MAE: 5.5726, RMSE: 7.9438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/1834357499.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4, Epoch 1: Train Loss 243.0483, Test Loss 205.9942\n",
      "Fold 4, Epoch 2: Train Loss 180.6908, Test Loss 108.8755\n",
      "Fold 4, Epoch 3: Train Loss 103.1256, Test Loss 100.0639\n",
      "Fold 4, Epoch 4: Train Loss 99.0422, Test Loss 97.5285\n",
      "Fold 4, Epoch 5: Train Loss 96.8618, Test Loss 95.2063\n",
      "Fold 4, Epoch 6: Train Loss 94.5352, Test Loss 91.9662\n",
      "Fold 4, Epoch 7: Train Loss 91.3491, Test Loss 89.2233\n",
      "Fold 4, Epoch 8: Train Loss 87.0911, Test Loss 83.4508\n",
      "Fold 4, Epoch 9: Train Loss 83.2681, Test Loss 83.4685\n",
      "Fold 4, Epoch 10: Train Loss 81.7285, Test Loss 79.5072\n",
      "Fold 4, Epoch 11: Train Loss 80.5225, Test Loss 79.4360\n",
      "Fold 4, Epoch 12: Train Loss 78.8539, Test Loss 77.5522\n",
      "Fold 4, Epoch 13: Train Loss 78.4052, Test Loss 82.4890\n",
      "Fold 4, Epoch 14: Train Loss 77.2487, Test Loss 75.5562\n",
      "Fold 4, Epoch 15: Train Loss 76.1423, Test Loss 75.7116\n",
      "Fold 4, Epoch 16: Train Loss 75.8263, Test Loss 81.9812\n",
      "Fold 4, Epoch 17: Train Loss 75.3391, Test Loss 73.7676\n",
      "Fold 4, Epoch 18: Train Loss 75.0243, Test Loss 74.2985\n",
      "Fold 4, Epoch 19: Train Loss 75.3481, Test Loss 75.7235\n",
      "Fold 4, Epoch 20: Train Loss 74.2912, Test Loss 72.9660\n",
      "Fold 4, Epoch 21: Train Loss 73.4375, Test Loss 73.0388\n",
      "Fold 4, Epoch 22: Train Loss 73.6318, Test Loss 73.9727\n",
      "Fold 4, Epoch 23: Train Loss 73.6012, Test Loss 77.4091\n",
      "Fold 4, Epoch 24: Train Loss 73.4102, Test Loss 72.6317\n",
      "Fold 4, Epoch 25: Train Loss 72.9578, Test Loss 72.0435\n",
      "Fold 4, Epoch 26: Train Loss 73.0027, Test Loss 72.3711\n",
      "Fold 4, Epoch 27: Train Loss 72.5629, Test Loss 73.8706\n",
      "Fold 4, Epoch 28: Train Loss 72.2820, Test Loss 71.1641\n",
      "Fold 4, Epoch 29: Train Loss 71.5397, Test Loss 72.1118\n",
      "Fold 4, Epoch 30: Train Loss 71.2720, Test Loss 72.0581\n",
      "Fold 4, Epoch 31: Train Loss 71.2863, Test Loss 71.9297\n",
      "Fold 4, Epoch 32: Train Loss 70.6939, Test Loss 72.3031\n",
      "Fold 4, Epoch 33: Train Loss 70.6885, Test Loss 70.9375\n",
      "Fold 4, Epoch 34: Train Loss 70.4310, Test Loss 70.3495\n",
      "Fold 4, Epoch 35: Train Loss 69.6834, Test Loss 71.5265\n",
      "Fold 4, Epoch 36: Train Loss 70.2855, Test Loss 71.3838\n",
      "Fold 4, Epoch 37: Train Loss 70.5639, Test Loss 71.6555\n",
      "Fold 4, Epoch 38: Train Loss 69.2767, Test Loss 71.7361\n",
      "Fold 4, Epoch 39: Train Loss 69.4191, Test Loss 69.8702\n",
      "Fold 4, Epoch 40: Train Loss 69.7174, Test Loss 69.8924\n",
      "Fold 4, Epoch 41: Train Loss 69.4679, Test Loss 69.4981\n",
      "Fold 4, Epoch 42: Train Loss 68.4772, Test Loss 69.4129\n",
      "Fold 4, Epoch 43: Train Loss 69.1195, Test Loss 69.7810\n",
      "Fold 4, Epoch 44: Train Loss 68.4830, Test Loss 69.9226\n",
      "Fold 4, Epoch 45: Train Loss 68.0835, Test Loss 69.6034\n",
      "Fold 4, Epoch 46: Train Loss 68.5651, Test Loss 69.0958\n",
      "Fold 4, Epoch 47: Train Loss 68.1843, Test Loss 68.6832\n",
      "Fold 4, Epoch 48: Train Loss 67.1657, Test Loss 68.2328\n",
      "Fold 4, Epoch 49: Train Loss 67.8849, Test Loss 72.5544\n",
      "Fold 4, Epoch 50: Train Loss 67.0414, Test Loss 67.9503\n",
      "Fold 4 - MAE: 5.6297, RMSE: 8.2468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/1834357499.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, Epoch 1: Train Loss 242.7775, Test Loss 204.6353\n",
      "Fold 5, Epoch 2: Train Loss 174.9928, Test Loss 104.6854\n",
      "Fold 5, Epoch 3: Train Loss 105.8649, Test Loss 95.8758\n",
      "Fold 5, Epoch 4: Train Loss 102.6593, Test Loss 92.7744\n",
      "Fold 5, Epoch 5: Train Loss 100.6980, Test Loss 91.1566\n",
      "Fold 5, Epoch 6: Train Loss 99.4794, Test Loss 93.4726\n",
      "Fold 5, Epoch 7: Train Loss 97.8100, Test Loss 84.3961\n",
      "Fold 5, Epoch 8: Train Loss 88.9980, Test Loss 75.7261\n",
      "Fold 5, Epoch 9: Train Loss 84.1075, Test Loss 77.1799\n",
      "Fold 5, Epoch 10: Train Loss 81.7014, Test Loss 73.3711\n",
      "Fold 5, Epoch 11: Train Loss 81.1437, Test Loss 72.5707\n",
      "Fold 5, Epoch 12: Train Loss 80.1805, Test Loss 72.0784\n",
      "Fold 5, Epoch 13: Train Loss 78.9102, Test Loss 71.2846\n",
      "Fold 5, Epoch 14: Train Loss 78.8404, Test Loss 73.4162\n",
      "Fold 5, Epoch 15: Train Loss 78.0587, Test Loss 69.8796\n",
      "Fold 5, Epoch 16: Train Loss 77.7573, Test Loss 78.4086\n",
      "Fold 5, Epoch 17: Train Loss 78.2069, Test Loss 71.8798\n",
      "Fold 5, Epoch 18: Train Loss 77.6299, Test Loss 69.4674\n",
      "Fold 5, Epoch 19: Train Loss 77.1747, Test Loss 69.0204\n",
      "Fold 5, Epoch 20: Train Loss 77.3632, Test Loss 72.1727\n",
      "Fold 5, Epoch 21: Train Loss 76.2364, Test Loss 70.5677\n",
      "Fold 5, Epoch 22: Train Loss 75.7326, Test Loss 68.6087\n",
      "Fold 5, Epoch 23: Train Loss 75.1742, Test Loss 68.9807\n",
      "Fold 5, Epoch 24: Train Loss 75.2237, Test Loss 68.1854\n",
      "Fold 5, Epoch 25: Train Loss 74.9763, Test Loss 72.1571\n",
      "Fold 5, Epoch 26: Train Loss 74.7856, Test Loss 71.6683\n",
      "Fold 5, Epoch 27: Train Loss 75.3616, Test Loss 68.3390\n",
      "Fold 5, Epoch 28: Train Loss 74.8586, Test Loss 68.3711\n",
      "Fold 5, Epoch 29: Train Loss 74.1785, Test Loss 68.3476\n",
      "Fold 5, Epoch 30: Train Loss 74.0630, Test Loss 71.9639\n",
      "Fold 5, Epoch 31: Train Loss 73.6711, Test Loss 69.0032\n",
      "Fold 5, Epoch 32: Train Loss 73.8769, Test Loss 67.9708\n",
      "Fold 5, Epoch 33: Train Loss 72.1502, Test Loss 69.7903\n",
      "Fold 5, Epoch 34: Train Loss 72.9761, Test Loss 67.2974\n",
      "Fold 5, Epoch 35: Train Loss 72.8988, Test Loss 67.7619\n",
      "Fold 5, Epoch 36: Train Loss 72.2900, Test Loss 67.3086\n",
      "Fold 5, Epoch 37: Train Loss 72.2351, Test Loss 66.7745\n",
      "Fold 5, Epoch 38: Train Loss 71.3856, Test Loss 67.5911\n",
      "Fold 5, Epoch 39: Train Loss 70.8523, Test Loss 66.0377\n",
      "Fold 5, Epoch 40: Train Loss 71.1586, Test Loss 66.7398\n",
      "Fold 5, Epoch 41: Train Loss 70.3866, Test Loss 65.3443\n",
      "Fold 5, Epoch 42: Train Loss 70.6004, Test Loss 65.5962\n",
      "Fold 5, Epoch 43: Train Loss 70.1139, Test Loss 66.5046\n",
      "Fold 5, Epoch 44: Train Loss 69.9938, Test Loss 65.6535\n",
      "Fold 5, Epoch 45: Train Loss 70.1450, Test Loss 65.4690\n",
      "Fold 5, Epoch 46: Train Loss 69.9198, Test Loss 67.4113\n",
      "Fold 5, Epoch 47: Train Loss 69.9057, Test Loss 64.9182\n",
      "Fold 5, Epoch 48: Train Loss 69.3257, Test Loss 65.0383\n",
      "Fold 5, Epoch 49: Train Loss 69.4044, Test Loss 65.9552\n",
      "Fold 5, Epoch 50: Train Loss 68.4875, Test Loss 67.2277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/1834357499.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 5.5868, RMSE: 8.0587\n",
      "\n",
      "Average MAE: 5.6075, Average RMSE: 8.1109\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # Set seed\n",
    "\n",
    "data = pd.read_csv('final_50_all.csv')\n",
    "\n",
    "num_features = ['temperature_2m (°C)', 'apparent_temperature (°C)', 'rain (mm)', 'wind_speed_100m (km/h)']\n",
    "\n",
    "# Feature engineering\n",
    "data['hour'] = pd.to_datetime(data['hour'])\n",
    "data['year'] = data['hour'].dt.year\n",
    "data['month'] = data['hour'].dt.month\n",
    "data['day'] = data['hour'].dt.day\n",
    "data['hour_of_day'] = data['hour'].dt.hour\n",
    "data['weekday'] = data['hour'].dt.weekday\n",
    "\n",
    "data['hour_sin'] = np.sin(2 * np.pi * data['hour_of_day'] / 24)\n",
    "data['hour_cos'] = np.cos(2 * np.pi * data['hour_of_day'] / 24)\n",
    "data['weekday_sin'] = np.sin(2 * np.pi * data['weekday'] / 7)\n",
    "data['weekday_cos'] = np.cos(2 * np.pi * data['weekday'] / 7)\n",
    "data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "\n",
    "# Scaling numerical features\n",
    "scaler = MinMaxScaler()\n",
    "data[num_features] = scaler.fit_transform(data[num_features])\n",
    "data.drop(['month','day','hour', 'hour_of_day', 'weekday'], axis=1, inplace=True)\n",
    "\n",
    "# Extracting features and target\n",
    "features = num_features + ['hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos','month_sin','month_cos']\n",
    "target = 'ride_count'\n",
    "\n",
    "X = data[features].values\n",
    "y = data[target].values\n",
    "\n",
    "# Normalizing data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create sequences for LSTM input\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(target[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 24  # Using the past 24 hours of data to predict\n",
    "X, y = create_sequences(X, y, sequence_length)\n",
    "\n",
    "# Implementing KFold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n",
    "\n",
    "    # Define the model\n",
    "    class GRUModelWithSimpleAttention(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size, dropout=0.2, attention_dropout=0.1):\n",
    "            super(GRUModelWithSimpleAttention, self).__init__()\n",
    "            self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "            self.attention_score = nn.Linear(hidden_size, 1)\n",
    "            self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gru_out, _ = self.gru(x)\n",
    "            attn_scores = self.attention_score(gru_out)\n",
    "            attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "            attn_weights = self.attention_dropout(attn_weights)\n",
    "            context_vector = torch.sum(attn_weights * gru_out, dim=1)\n",
    "            out = self.dropout(context_vector)\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = GRUModelWithSimpleAttention(input_size=len(features), hidden_size=64, output_size=1, dropout=0.2, attention_dropout=0.1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience, trials = 10, 0\n",
    "\n",
    "    for epoch in range(50):  # Reduced number of epochs for brevity\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Testing model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                test_loss += criterion(y_pred, y_batch).item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1}: Train Loss {train_loss:.4f}, Test Loss {test_loss:.4f}\")\n",
    "\n",
    "        if test_loss < best_val_loss:\n",
    "            best_val_loss = test_loss\n",
    "            trials = 0\n",
    "            torch.save(model.state_dict(), f'model_fold_{fold+1}.pth')\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "\n",
    "    # Load the best model and evaluate on the test set\n",
    "    model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    y_preds, y_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            y_preds.extend(y_pred.tolist())\n",
    "            y_trues.extend(y_batch.tolist())\n",
    "\n",
    "    mae = mean_absolute_error(y_trues, y_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "    results.append((mae, rmse))\n",
    "    print(f\"Fold {fold+1} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Calculate average MAE and RMSE across folds\n",
    "avg_mae = np.mean([res[0] for res in results])\n",
    "avg_rmse = np.mean([res[1] for res in results])\n",
    "print(f\"\\nAverage MAE: {avg_mae:.4f}, Average RMSE: {avg_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e334a319-f9a1-4b5e-9af5-317e1fd81d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 1: Train Loss 246.5969, Test Loss 197.8791\n",
      "Fold 1, Epoch 2: Train Loss 194.1516, Test Loss 128.5456\n",
      "Fold 1, Epoch 3: Train Loss 126.6488, Test Loss 108.9884\n",
      "Fold 1, Epoch 4: Train Loss 120.4667, Test Loss 106.2424\n",
      "Fold 1, Epoch 5: Train Loss 117.9492, Test Loss 104.3088\n",
      "Fold 1, Epoch 6: Train Loss 116.3518, Test Loss 102.8310\n",
      "Fold 1, Epoch 7: Train Loss 115.0033, Test Loss 101.3728\n",
      "Fold 1, Epoch 8: Train Loss 113.6009, Test Loss 99.6148\n",
      "Fold 1, Epoch 9: Train Loss 111.9644, Test Loss 96.8459\n",
      "Fold 1, Epoch 10: Train Loss 108.4578, Test Loss 91.8248\n",
      "Fold 1, Epoch 11: Train Loss 101.9110, Test Loss 80.6201\n",
      "Fold 1, Epoch 12: Train Loss 89.8171, Test Loss 66.8473\n",
      "Fold 1, Epoch 13: Train Loss 81.5825, Test Loss 62.5950\n",
      "Fold 1, Epoch 14: Train Loss 79.3319, Test Loss 61.6651\n",
      "Fold 1, Epoch 15: Train Loss 77.4616, Test Loss 59.5131\n",
      "Fold 1, Epoch 16: Train Loss 76.0083, Test Loss 61.3381\n",
      "Fold 1, Epoch 17: Train Loss 75.6368, Test Loss 61.4862\n",
      "Fold 1, Epoch 18: Train Loss 75.3172, Test Loss 63.4535\n",
      "Fold 1, Epoch 19: Train Loss 74.9774, Test Loss 58.8429\n",
      "Fold 1, Epoch 20: Train Loss 74.3084, Test Loss 57.7585\n",
      "Fold 1, Epoch 21: Train Loss 73.7912, Test Loss 57.9105\n",
      "Fold 1, Epoch 22: Train Loss 73.3374, Test Loss 59.1223\n",
      "Fold 1, Epoch 23: Train Loss 73.1909, Test Loss 56.6796\n",
      "Fold 1, Epoch 24: Train Loss 72.2475, Test Loss 60.1761\n",
      "Fold 1, Epoch 25: Train Loss 72.0482, Test Loss 58.7121\n",
      "Fold 1, Epoch 26: Train Loss 72.1349, Test Loss 56.3955\n",
      "Fold 1, Epoch 27: Train Loss 71.4459, Test Loss 60.7725\n",
      "Fold 1, Epoch 28: Train Loss 71.4501, Test Loss 59.3196\n",
      "Fold 1, Epoch 29: Train Loss 70.6458, Test Loss 56.4463\n",
      "Fold 1, Epoch 30: Train Loss 70.6561, Test Loss 58.1387\n",
      "Fold 1, Epoch 31: Train Loss 69.7434, Test Loss 59.3321\n",
      "Fold 1, Epoch 32: Train Loss 69.4384, Test Loss 56.0962\n",
      "Fold 1, Epoch 33: Train Loss 69.4504, Test Loss 55.5512\n",
      "Fold 1, Epoch 34: Train Loss 69.9105, Test Loss 55.7436\n",
      "Fold 1, Epoch 35: Train Loss 69.6128, Test Loss 56.0393\n",
      "Fold 1, Epoch 36: Train Loss 68.7117, Test Loss 54.6538\n",
      "Fold 1, Epoch 37: Train Loss 68.5814, Test Loss 57.4256\n",
      "Fold 1, Epoch 38: Train Loss 68.0512, Test Loss 55.1345\n",
      "Fold 1, Epoch 39: Train Loss 67.2007, Test Loss 58.4093\n",
      "Fold 1, Epoch 40: Train Loss 68.0320, Test Loss 53.6094\n",
      "Fold 1, Epoch 41: Train Loss 67.2835, Test Loss 53.2401\n",
      "Fold 1, Epoch 42: Train Loss 66.8773, Test Loss 55.4598\n",
      "Fold 1, Epoch 43: Train Loss 66.9666, Test Loss 52.9085\n",
      "Fold 1, Epoch 44: Train Loss 65.7323, Test Loss 56.6289\n",
      "Fold 1, Epoch 45: Train Loss 66.1787, Test Loss 54.3177\n",
      "Fold 1, Epoch 46: Train Loss 65.8701, Test Loss 53.7374\n",
      "Fold 1, Epoch 47: Train Loss 65.5080, Test Loss 52.9510\n",
      "Fold 1, Epoch 48: Train Loss 64.8426, Test Loss 56.6560\n",
      "Fold 1, Epoch 49: Train Loss 65.1270, Test Loss 52.7140\n",
      "Fold 1, Epoch 50: Train Loss 64.5269, Test Loss 53.6574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/2435093727.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - MAE: 5.1495, RMSE: 7.2557\n",
      "Fold 2, Epoch 1: Train Loss 240.4557, Test Loss 201.5645\n",
      "Fold 2, Epoch 2: Train Loss 182.4447, Test Loss 120.1365\n",
      "Fold 2, Epoch 3: Train Loss 121.9679, Test Loss 113.8813\n",
      "Fold 2, Epoch 4: Train Loss 119.8112, Test Loss 111.1556\n",
      "Fold 2, Epoch 5: Train Loss 116.7142, Test Loss 110.3356\n",
      "Fold 2, Epoch 6: Train Loss 115.5272, Test Loss 108.1941\n",
      "Fold 2, Epoch 7: Train Loss 114.0943, Test Loss 106.8793\n",
      "Fold 2, Epoch 8: Train Loss 112.1882, Test Loss 106.4357\n",
      "Fold 2, Epoch 9: Train Loss 110.4545, Test Loss 101.2380\n",
      "Fold 2, Epoch 10: Train Loss 105.7330, Test Loss 91.6079\n",
      "Fold 2, Epoch 11: Train Loss 92.5636, Test Loss 78.0291\n",
      "Fold 2, Epoch 12: Train Loss 81.9776, Test Loss 70.0950\n",
      "Fold 2, Epoch 13: Train Loss 79.5420, Test Loss 69.5266\n",
      "Fold 2, Epoch 14: Train Loss 78.8071, Test Loss 69.0139\n",
      "Fold 2, Epoch 15: Train Loss 76.7233, Test Loss 66.7565\n",
      "Fold 2, Epoch 16: Train Loss 75.2259, Test Loss 67.7580\n",
      "Fold 2, Epoch 17: Train Loss 75.6257, Test Loss 65.8087\n",
      "Fold 2, Epoch 18: Train Loss 74.8813, Test Loss 66.3406\n",
      "Fold 2, Epoch 19: Train Loss 74.3908, Test Loss 65.4240\n",
      "Fold 2, Epoch 20: Train Loss 74.1276, Test Loss 64.3786\n",
      "Fold 2, Epoch 21: Train Loss 72.5332, Test Loss 65.2452\n",
      "Fold 2, Epoch 22: Train Loss 73.1589, Test Loss 67.7737\n",
      "Fold 2, Epoch 23: Train Loss 72.9237, Test Loss 64.7245\n",
      "Fold 2, Epoch 24: Train Loss 71.1197, Test Loss 63.2816\n",
      "Fold 2, Epoch 25: Train Loss 71.3816, Test Loss 64.2599\n",
      "Fold 2, Epoch 26: Train Loss 71.4300, Test Loss 63.6464\n",
      "Fold 2, Epoch 27: Train Loss 70.9671, Test Loss 63.1535\n",
      "Fold 2, Epoch 28: Train Loss 70.3719, Test Loss 63.3204\n",
      "Fold 2, Epoch 29: Train Loss 70.3151, Test Loss 63.1599\n",
      "Fold 2, Epoch 30: Train Loss 69.5926, Test Loss 62.6536\n",
      "Fold 2, Epoch 31: Train Loss 68.9691, Test Loss 62.8743\n",
      "Fold 2, Epoch 32: Train Loss 68.7093, Test Loss 65.2822\n",
      "Fold 2, Epoch 33: Train Loss 68.6007, Test Loss 62.3228\n",
      "Fold 2, Epoch 34: Train Loss 68.1797, Test Loss 62.5591\n",
      "Fold 2, Epoch 35: Train Loss 67.2671, Test Loss 62.8618\n",
      "Fold 2, Epoch 36: Train Loss 68.0147, Test Loss 62.7590\n",
      "Fold 2, Epoch 37: Train Loss 67.3613, Test Loss 66.7776\n",
      "Fold 2, Epoch 38: Train Loss 67.8166, Test Loss 61.0192\n",
      "Fold 2, Epoch 39: Train Loss 66.3608, Test Loss 62.2507\n",
      "Fold 2, Epoch 40: Train Loss 66.4423, Test Loss 62.7178\n",
      "Fold 2, Epoch 41: Train Loss 66.4834, Test Loss 60.6467\n",
      "Fold 2, Epoch 42: Train Loss 66.2712, Test Loss 62.4644\n",
      "Fold 2, Epoch 43: Train Loss 65.8761, Test Loss 59.0777\n",
      "Fold 2, Epoch 44: Train Loss 64.9574, Test Loss 58.8018\n",
      "Fold 2, Epoch 45: Train Loss 65.0714, Test Loss 59.1062\n",
      "Fold 2, Epoch 46: Train Loss 64.6511, Test Loss 60.1612\n",
      "Fold 2, Epoch 47: Train Loss 63.8574, Test Loss 62.3232\n",
      "Fold 2, Epoch 48: Train Loss 63.7290, Test Loss 60.5261\n",
      "Fold 2, Epoch 49: Train Loss 63.8659, Test Loss 60.6626\n",
      "Fold 2, Epoch 50: Train Loss 63.8565, Test Loss 60.7121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/2435093727.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - MAE: 5.2849, RMSE: 7.6729\n",
      "Fold 3, Epoch 1: Train Loss 244.4514, Test Loss 204.9531\n",
      "Fold 3, Epoch 2: Train Loss 179.4778, Test Loss 124.1562\n",
      "Fold 3, Epoch 3: Train Loss 122.1479, Test Loss 116.6795\n",
      "Fold 3, Epoch 4: Train Loss 118.1492, Test Loss 112.0637\n",
      "Fold 3, Epoch 5: Train Loss 116.4660, Test Loss 110.6533\n",
      "Fold 3, Epoch 6: Train Loss 115.5651, Test Loss 109.2712\n",
      "Fold 3, Epoch 7: Train Loss 113.8539, Test Loss 106.4856\n",
      "Fold 3, Epoch 8: Train Loss 111.9072, Test Loss 104.3016\n",
      "Fold 3, Epoch 9: Train Loss 107.1763, Test Loss 96.7952\n",
      "Fold 3, Epoch 10: Train Loss 90.7012, Test Loss 71.8133\n",
      "Fold 3, Epoch 11: Train Loss 80.8075, Test Loss 68.3148\n",
      "Fold 3, Epoch 12: Train Loss 78.5638, Test Loss 72.4981\n",
      "Fold 3, Epoch 13: Train Loss 77.2641, Test Loss 65.8621\n",
      "Fold 3, Epoch 14: Train Loss 77.4474, Test Loss 66.3391\n",
      "Fold 3, Epoch 15: Train Loss 75.3180, Test Loss 66.1287\n",
      "Fold 3, Epoch 16: Train Loss 74.9122, Test Loss 64.5300\n",
      "Fold 3, Epoch 17: Train Loss 75.1789, Test Loss 64.2738\n",
      "Fold 3, Epoch 18: Train Loss 74.1641, Test Loss 65.4463\n",
      "Fold 3, Epoch 19: Train Loss 73.0955, Test Loss 63.5685\n",
      "Fold 3, Epoch 20: Train Loss 72.9795, Test Loss 63.0233\n",
      "Fold 3, Epoch 21: Train Loss 72.8901, Test Loss 63.9924\n",
      "Fold 3, Epoch 22: Train Loss 72.2917, Test Loss 62.5043\n",
      "Fold 3, Epoch 23: Train Loss 71.9758, Test Loss 62.3274\n",
      "Fold 3, Epoch 24: Train Loss 71.4640, Test Loss 62.5770\n",
      "Fold 3, Epoch 25: Train Loss 71.9332, Test Loss 62.1610\n",
      "Fold 3, Epoch 26: Train Loss 70.1779, Test Loss 61.8570\n",
      "Fold 3, Epoch 27: Train Loss 70.2217, Test Loss 62.0458\n",
      "Fold 3, Epoch 28: Train Loss 70.2041, Test Loss 60.6911\n",
      "Fold 3, Epoch 29: Train Loss 69.3338, Test Loss 60.6043\n",
      "Fold 3, Epoch 30: Train Loss 69.2430, Test Loss 59.7139\n",
      "Fold 3, Epoch 31: Train Loss 68.9681, Test Loss 60.0612\n",
      "Fold 3, Epoch 32: Train Loss 68.6000, Test Loss 62.2974\n",
      "Fold 3, Epoch 33: Train Loss 69.1644, Test Loss 61.9265\n",
      "Fold 3, Epoch 34: Train Loss 67.4664, Test Loss 60.8573\n",
      "Fold 3, Epoch 35: Train Loss 67.0121, Test Loss 62.4173\n",
      "Fold 3, Epoch 36: Train Loss 67.6036, Test Loss 59.7696\n",
      "Fold 3, Epoch 37: Train Loss 67.7720, Test Loss 60.9323\n",
      "Fold 3, Epoch 38: Train Loss 66.9458, Test Loss 60.7219\n",
      "Fold 3, Epoch 39: Train Loss 67.3389, Test Loss 60.4604\n",
      "Fold 3, Epoch 40: Train Loss 66.5768, Test Loss 59.8849\n",
      "Early stopping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/2435093727.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - MAE: 5.4974, RMSE: 7.7234\n",
      "Fold 4, Epoch 1: Train Loss 241.5352, Test Loss 207.0091\n",
      "Fold 4, Epoch 2: Train Loss 204.5371, Test Loss 156.8512\n",
      "Fold 4, Epoch 3: Train Loss 126.7606, Test Loss 116.3864\n",
      "Fold 4, Epoch 4: Train Loss 117.0100, Test Loss 112.3857\n",
      "Fold 4, Epoch 5: Train Loss 113.9442, Test Loss 111.4023\n",
      "Fold 4, Epoch 6: Train Loss 113.1145, Test Loss 109.2485\n",
      "Fold 4, Epoch 7: Train Loss 111.5193, Test Loss 107.1660\n",
      "Fold 4, Epoch 8: Train Loss 109.1143, Test Loss 111.7974\n",
      "Fold 4, Epoch 9: Train Loss 106.8380, Test Loss 103.8346\n",
      "Fold 4, Epoch 10: Train Loss 105.3968, Test Loss 102.4794\n",
      "Fold 4, Epoch 11: Train Loss 101.1129, Test Loss 90.7004\n",
      "Fold 4, Epoch 12: Train Loss 85.2256, Test Loss 74.6668\n",
      "Fold 4, Epoch 13: Train Loss 80.2596, Test Loss 73.7686\n",
      "Fold 4, Epoch 14: Train Loss 77.8933, Test Loss 70.3764\n",
      "Fold 4, Epoch 15: Train Loss 75.8752, Test Loss 68.4138\n",
      "Fold 4, Epoch 16: Train Loss 75.0752, Test Loss 68.3875\n",
      "Fold 4, Epoch 17: Train Loss 73.7213, Test Loss 68.9804\n",
      "Fold 4, Epoch 18: Train Loss 73.4328, Test Loss 68.3438\n",
      "Fold 4, Epoch 19: Train Loss 73.1284, Test Loss 65.7809\n",
      "Fold 4, Epoch 20: Train Loss 73.0692, Test Loss 66.1805\n",
      "Fold 4, Epoch 21: Train Loss 72.8996, Test Loss 66.1803\n",
      "Fold 4, Epoch 22: Train Loss 71.2110, Test Loss 65.9707\n",
      "Fold 4, Epoch 23: Train Loss 70.9102, Test Loss 66.6019\n",
      "Fold 4, Epoch 24: Train Loss 70.5945, Test Loss 66.6943\n",
      "Fold 4, Epoch 25: Train Loss 70.1690, Test Loss 65.6725\n",
      "Fold 4, Epoch 26: Train Loss 70.2854, Test Loss 65.3384\n",
      "Fold 4, Epoch 27: Train Loss 69.6621, Test Loss 63.4491\n",
      "Fold 4, Epoch 28: Train Loss 69.6658, Test Loss 63.6700\n",
      "Fold 4, Epoch 29: Train Loss 69.1264, Test Loss 63.2577\n",
      "Fold 4, Epoch 30: Train Loss 67.9578, Test Loss 64.6461\n",
      "Fold 4, Epoch 31: Train Loss 68.6153, Test Loss 62.4173\n",
      "Fold 4, Epoch 32: Train Loss 68.5375, Test Loss 62.7745\n",
      "Fold 4, Epoch 33: Train Loss 68.0321, Test Loss 63.5729\n",
      "Fold 4, Epoch 34: Train Loss 67.0777, Test Loss 62.3468\n",
      "Fold 4, Epoch 35: Train Loss 67.2230, Test Loss 62.1878\n",
      "Fold 4, Epoch 36: Train Loss 66.1934, Test Loss 63.0501\n",
      "Fold 4, Epoch 37: Train Loss 66.2758, Test Loss 63.2199\n",
      "Fold 4, Epoch 38: Train Loss 66.2200, Test Loss 63.1716\n",
      "Fold 4, Epoch 39: Train Loss 66.1197, Test Loss 63.4567\n",
      "Fold 4, Epoch 40: Train Loss 65.8776, Test Loss 61.3126\n",
      "Fold 4, Epoch 41: Train Loss 65.6385, Test Loss 62.0521\n",
      "Fold 4, Epoch 42: Train Loss 65.9797, Test Loss 62.7335\n",
      "Fold 4, Epoch 43: Train Loss 65.3507, Test Loss 65.5107\n",
      "Fold 4, Epoch 44: Train Loss 64.5099, Test Loss 67.0507\n",
      "Fold 4, Epoch 45: Train Loss 64.9326, Test Loss 62.2654\n",
      "Fold 4, Epoch 46: Train Loss 64.2083, Test Loss 60.9648\n",
      "Fold 4, Epoch 47: Train Loss 64.0600, Test Loss 60.3387\n",
      "Fold 4, Epoch 48: Train Loss 64.5335, Test Loss 61.5223\n",
      "Fold 4, Epoch 49: Train Loss 63.5495, Test Loss 60.4023\n",
      "Fold 4, Epoch 50: Train Loss 63.8177, Test Loss 61.1174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/2435093727.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - MAE: 5.4267, RMSE: 7.7720\n",
      "Fold 5, Epoch 1: Train Loss 233.7148, Test Loss 221.2677\n",
      "Fold 5, Epoch 2: Train Loss 178.2826, Test Loss 135.5777\n",
      "Fold 5, Epoch 3: Train Loss 117.7179, Test Loss 127.4925\n",
      "Fold 5, Epoch 4: Train Loss 113.8052, Test Loss 125.5883\n",
      "Fold 5, Epoch 5: Train Loss 112.5535, Test Loss 125.6906\n",
      "Fold 5, Epoch 6: Train Loss 111.0757, Test Loss 122.6900\n",
      "Fold 5, Epoch 7: Train Loss 110.7365, Test Loss 121.7621\n",
      "Fold 5, Epoch 8: Train Loss 109.0269, Test Loss 119.3967\n",
      "Fold 5, Epoch 9: Train Loss 106.5963, Test Loss 116.9749\n",
      "Fold 5, Epoch 10: Train Loss 103.9335, Test Loss 114.6007\n",
      "Fold 5, Epoch 11: Train Loss 93.1312, Test Loss 91.7045\n",
      "Fold 5, Epoch 12: Train Loss 78.6106, Test Loss 92.0234\n",
      "Fold 5, Epoch 13: Train Loss 75.0673, Test Loss 83.6682\n",
      "Fold 5, Epoch 14: Train Loss 73.8673, Test Loss 82.2251\n",
      "Fold 5, Epoch 15: Train Loss 72.0047, Test Loss 81.0272\n",
      "Fold 5, Epoch 16: Train Loss 70.4805, Test Loss 81.6624\n",
      "Fold 5, Epoch 17: Train Loss 70.4074, Test Loss 79.4798\n",
      "Fold 5, Epoch 18: Train Loss 69.5958, Test Loss 80.8059\n",
      "Fold 5, Epoch 19: Train Loss 69.6214, Test Loss 80.4891\n",
      "Fold 5, Epoch 20: Train Loss 68.7317, Test Loss 80.3343\n",
      "Fold 5, Epoch 21: Train Loss 68.8027, Test Loss 78.2967\n",
      "Fold 5, Epoch 22: Train Loss 68.1321, Test Loss 81.1852\n",
      "Fold 5, Epoch 23: Train Loss 67.3903, Test Loss 79.7481\n",
      "Fold 5, Epoch 24: Train Loss 66.8726, Test Loss 77.8130\n",
      "Fold 5, Epoch 25: Train Loss 66.7066, Test Loss 77.3259\n",
      "Fold 5, Epoch 26: Train Loss 65.8253, Test Loss 76.7932\n",
      "Fold 5, Epoch 27: Train Loss 66.2539, Test Loss 80.2522\n",
      "Fold 5, Epoch 28: Train Loss 65.1502, Test Loss 77.1264\n",
      "Fold 5, Epoch 29: Train Loss 65.4585, Test Loss 78.5125\n",
      "Fold 5, Epoch 30: Train Loss 65.0726, Test Loss 75.7600\n",
      "Fold 5, Epoch 31: Train Loss 65.0862, Test Loss 74.8096\n",
      "Fold 5, Epoch 32: Train Loss 64.5845, Test Loss 78.4842\n",
      "Fold 5, Epoch 33: Train Loss 64.5209, Test Loss 73.9845\n",
      "Fold 5, Epoch 34: Train Loss 64.5441, Test Loss 75.8389\n",
      "Fold 5, Epoch 35: Train Loss 63.5472, Test Loss 75.8412\n",
      "Fold 5, Epoch 36: Train Loss 64.0820, Test Loss 75.0410\n",
      "Fold 5, Epoch 37: Train Loss 63.0736, Test Loss 74.7983\n",
      "Fold 5, Epoch 38: Train Loss 62.7928, Test Loss 73.5804\n",
      "Fold 5, Epoch 39: Train Loss 62.7039, Test Loss 73.4634\n",
      "Fold 5, Epoch 40: Train Loss 62.5054, Test Loss 74.0765\n",
      "Fold 5, Epoch 41: Train Loss 62.7805, Test Loss 73.1071\n",
      "Fold 5, Epoch 42: Train Loss 62.0366, Test Loss 72.4589\n",
      "Fold 5, Epoch 43: Train Loss 62.2656, Test Loss 73.0916\n",
      "Fold 5, Epoch 44: Train Loss 61.4115, Test Loss 72.9220\n",
      "Fold 5, Epoch 45: Train Loss 62.1505, Test Loss 74.4058\n",
      "Fold 5, Epoch 46: Train Loss 61.8042, Test Loss 72.6860\n",
      "Fold 5, Epoch 47: Train Loss 60.8661, Test Loss 73.5771\n",
      "Fold 5, Epoch 48: Train Loss 60.1436, Test Loss 71.8236\n",
      "Fold 5, Epoch 49: Train Loss 60.2477, Test Loss 74.0977\n",
      "Fold 5, Epoch 50: Train Loss 59.8774, Test Loss 71.9515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bg/rj2597hd4399k2ht0hpblk9r0000gn/T/ipykernel_90635/2435093727.py:152: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - MAE: 5.5080, RMSE: 8.4807\n",
      "\n",
      "Average MAE: 5.3733, Average RMSE: 7.7809\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "# Setting random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)  # Set seed\n",
    "\n",
    "data = pd.read_csv('final_50_all.csv')\n",
    "\n",
    "num_features = ['temperature_2m (°C)', 'apparent_temperature (°C)', 'rain (mm)', 'wind_speed_100m (km/h)']\n",
    "\n",
    "# Feature engineering\n",
    "data['hour'] = pd.to_datetime(data['hour'])\n",
    "data['year'] = data['hour'].dt.year\n",
    "data['month'] = data['hour'].dt.month\n",
    "data['day'] = data['hour'].dt.day\n",
    "data['hour_of_day'] = data['hour'].dt.hour\n",
    "data['weekday'] = data['hour'].dt.weekday\n",
    "\n",
    "data['hour_sin'] = np.sin(2 * np.pi * data['hour_of_day'] / 24)\n",
    "data['hour_cos'] = np.cos(2 * np.pi * data['hour_of_day'] / 24)\n",
    "data['weekday_sin'] = np.sin(2 * np.pi * data['weekday'] / 7)\n",
    "data['weekday_cos'] = np.cos(2 * np.pi * data['weekday'] / 7)\n",
    "data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "\n",
    "# Scaling numerical features\n",
    "scaler = MinMaxScaler()\n",
    "data[num_features] = scaler.fit_transform(data[num_features])\n",
    "data.drop(['month','day','hour', 'hour_of_day', 'weekday'], axis=1, inplace=True)\n",
    "\n",
    "# Extracting features and target\n",
    "features = num_features + ['hour_sin', 'hour_cos', 'weekday_sin', 'weekday_cos','month_sin','month_cos']\n",
    "target = 'ride_count'\n",
    "\n",
    "X = data[features].values\n",
    "y = data[target].values\n",
    "\n",
    "# Normalizing data\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Create sequences for LSTM input\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X.append(data[i:i + sequence_length])\n",
    "        y.append(target[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 48  # Using the past 24 hours of data to predict\n",
    "X, y = create_sequences(X, y, sequence_length)\n",
    "\n",
    "# Implementing KFold Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    # Loaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=32, shuffle=False)\n",
    "\n",
    "    # Define the model\n",
    "    class GRUModelWithSimpleAttention(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, output_size, dropout=0.2, attention_dropout=0.1):\n",
    "            super(GRUModelWithSimpleAttention, self).__init__()\n",
    "            self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "            self.attention_score = nn.Linear(hidden_size, 1)\n",
    "            self.attention_dropout = nn.Dropout(attention_dropout)\n",
    "            self.fc = nn.Linear(hidden_size, output_size)\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        def forward(self, x):\n",
    "            gru_out, _ = self.gru(x)\n",
    "            attn_scores = self.attention_score(gru_out)\n",
    "            attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "            attn_weights = self.attention_dropout(attn_weights)\n",
    "            context_vector = torch.sum(attn_weights * gru_out, dim=1)\n",
    "            out = self.dropout(context_vector)\n",
    "            out = self.fc(out)\n",
    "            return out\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = GRUModelWithSimpleAttention(input_size=len(features), hidden_size=64, output_size=1, dropout=0.2, attention_dropout=0.1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience, trials = 10, 0\n",
    "\n",
    "    for epoch in range(50):  # Reduced number of epochs for brevity\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Testing model\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                y_pred = model(X_batch).squeeze()\n",
    "                test_loss += criterion(y_pred, y_batch).item()\n",
    "\n",
    "        test_loss /= len(test_loader)\n",
    "        print(f\"Fold {fold+1}, Epoch {epoch+1}: Train Loss {train_loss:.4f}, Test Loss {test_loss:.4f}\")\n",
    "\n",
    "        if test_loss < best_val_loss:\n",
    "            best_val_loss = test_loss\n",
    "            trials = 0\n",
    "            torch.save(model.state_dict(), f'model_fold_{fold+1}.pth')\n",
    "        else:\n",
    "            trials += 1\n",
    "            if trials >= patience:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "\n",
    "    # Load the best model and evaluate on the test set\n",
    "    model.load_state_dict(torch.load(f'model_fold_{fold+1}.pth'))\n",
    "    model.eval()\n",
    "    y_preds, y_trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch).squeeze()\n",
    "            y_preds.extend(y_pred.tolist())\n",
    "            y_trues.extend(y_batch.tolist())\n",
    "\n",
    "    mae = mean_absolute_error(y_trues, y_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "    results.append((mae, rmse))\n",
    "    print(f\"Fold {fold+1} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Calculate average MAE and RMSE across folds\n",
    "avg_mae = np.mean([res[0] for res in results])\n",
    "avg_rmse = np.mean([res[1] for res in results])\n",
    "print(f\"\\nAverage MAE: {avg_mae:.4f}, Average RMSE: {avg_rmse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
